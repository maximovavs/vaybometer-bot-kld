#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
gpt.py

–õ–æ–≥–∏–∫–∞ –¥–ª—è –±–ª–æ–∫–∞ ¬´–í—ã–≤–æ–¥¬ª –∏ ¬´–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏¬ª + –æ–±—â–∞—è –æ–±—ë—Ä—Ç–∫–∞ gpt_complete().

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è (–æ–±–Ω–æ–≤–ª–µ–Ω–æ):
‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: OpenAI ‚Üí Gemini ‚Üí Groq.
‚Ä¢ OpenAI: –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å (–±–µ–∑ —Ä–µ—Ç—Ä–∞–µ–≤). –ù–∞ 429 / insufficient_quota ‚Äî —Å—Ä–∞–∑—É —É—Ö–æ–¥–∏–º –¥–∞–ª—å—à–µ.
‚Ä¢ Gemini: –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º—É —Å–ø–∏—Å–∫—É (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ENV).
  –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç Gemini (base_url=.../v1beta/openai/).
‚Ä¢ Groq: –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ, –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª–∏ –ø–æ —Å–ø–∏—Å–∫—É.

ENV:
  OPENAI_API_KEY, GEMINI_API_KEY, GROQ_API_KEY
  OPENAI_MODEL (default: gpt-4o-mini)
  GEMINI_MODELS (comma-separated list; default: see GEMINI_MODELS_DEFAULT)
  PROVIDER_ORDER (optional, comma-separated: openai,gemini,groq)
"""

from __future__ import annotations

import logging
import os
import random
from typing import List, Optional, Tuple, Iterable, Set

log = logging.getLogger(__name__)

try:
    from openai import OpenAI  # type: ignore
except Exception:  # ImportError / RuntimeError etc.
    OpenAI = None  # type: ignore


# ‚îÄ‚îÄ –∫–ª—é—á–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
OPENAI_KEY = (os.getenv("OPENAI_API_KEY") or "").strip()
GEMINI_KEY = (os.getenv("GEMINI_API_KEY") or "").strip()
GROQ_KEY = (os.getenv("GROQ_API_KEY") or "").strip()

OPENAI_MODEL = (os.getenv("OPENAI_MODEL") or "gpt-4o-mini").strip()

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–æ–¥–µ–ª–µ–π Gemini (–ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
GEMINI_MODELS_DEFAULT = [
    "gemini-3-flash",
    "gemini-3-pro",
    "gemini-2.5-flash",
    "gemini-3-flash-preview",
]

# –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ENV PROVIDER_ORDER="openai,gemini,groq")
_PROVIDER_ENV = (os.getenv("PROVIDER_ORDER") or "").strip()
if _PROVIDER_ENV:
    PROVIDER_ORDER = [p.strip().lower() for p in _PROVIDER_ENV.split(",") if p.strip()]
else:
    PROVIDER_ORDER = ["openai", "gemini", "groq"]

# –ú–æ–¥–µ–ª–∏ Groq (–ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è —Å—Ä–∞–±–æ—Ç–∞–µ—Ç)
GROQ_MODELS = [
    "moonshotai/kimi-k2-instruct-0905",
    "llama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    "gemma2-9b-it",
    "qwen/qwen3-32b",
    "deepseek-r1-distill-llama-70b",
]

# ‚îÄ‚îÄ –∫–µ—à –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å 404 –ø–æ –∫—Ä—É–≥—É) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_GEMINI_AVAILABLE: Optional[Set[str]] = None

def _gemini_available_models(cli: "OpenAI") -> Optional[Set[str]]:
    """–ü—ã—Ç–∞–µ–º—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Gemini —á–µ—Ä–µ–∑ models.list()."""
    global _GEMINI_AVAILABLE
    if _GEMINI_AVAILABLE is not None:
        return _GEMINI_AVAILABLE
    try:
        resp = cli.models.list()
        ids = set()
        for m in getattr(resp, "data", []) or []:
            mid = getattr(m, "id", None)
            if isinstance(mid, str) and mid.strip():
                ids.add(mid.strip())
        _GEMINI_AVAILABLE = ids or set()
        if _GEMINI_AVAILABLE:
            log.info("Gemini models.list(): %d models", len(_GEMINI_AVAILABLE))
        return _GEMINI_AVAILABLE
    except Exception as e:
        log.warning("Gemini models.list() failed (will try by probing): %s", e)
        _GEMINI_AVAILABLE = set()  # –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ ¬´–Ω–µ —É–¥–∞–ª–æ—Å—å/–ø—É—Å—Ç–æ¬ª, —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–≥–∞—Ç—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ
        return None

# ‚îÄ‚îÄ –∫–ª–∏–µ–Ω—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _openai_client() -> Optional["OpenAI"]:
    """–ö–ª–∏–µ–Ω—Ç OpenAI —Å –æ—Ç–∫–ª—é—á—ë–Ω–Ω—ã–º–∏ —Ä–µ—Ç—Ä–∞—è–º–∏."""
    if not OPENAI_KEY or not OpenAI:
        return None
    try:
        return OpenAI(api_key=OPENAI_KEY, timeout=20.0, max_retries=0)
    except Exception as e:
        log.warning("OpenAI client init error: %s", e)
        return None


def _gemini_client() -> Optional["OpenAI"]:
    """OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç Gemini."""
    if not GEMINI_KEY or not OpenAI:
        return None
    try:
        return OpenAI(
            api_key=GEMINI_KEY,
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            timeout=25.0,
            max_retries=0,
        )
    except Exception as e:
        log.warning("Gemini client init error: %s", e)
        return None


def _groq_client() -> Optional["OpenAI"]:
    """OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç Groq."""
    if not GROQ_KEY or not OpenAI:
        return None
    try:
        return OpenAI(
            api_key=GROQ_KEY,
            base_url="https://api.groq.com/openai/v1",
            timeout=25.0,
            max_retries=0,
        )
    except Exception as e:
        log.warning("Groq client init error: %s", e)
        return None


# ‚îÄ‚îÄ —É—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _is_quota_or_ratelimit(err_text: str) -> bool:
    s = (err_text or "").lower()
    return any(k in s for k in (
        "insufficient_quota",
        "quota",
        "rate limit",
        "ratelimit",
        "429",
        "too many requests",
    ))


def _is_model_not_found(err_text: str) -> bool:
    s = (err_text or "").lower()
    return any(k in s for k in (
        "not found",
        "model not found",
        "does not exist",
        "unsupported",
        "404",
        "decommissioned",
        "invalid model",
    ))


def _unique_keep_order(items: Iterable[str]) -> List[str]:
    seen = set()
    out: List[str] = []
    for x in items:
        x = (x or "").strip()
        if not x or x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _gemini_models_to_try() -> List[str]:
    """–ë–µ—Ä—ë–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Gemini –∏–∑ ENV (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω), –∏–Ω–∞—á–µ ‚Äî –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π."""
    env_list = (os.getenv("GEMINI_MODELS") or "").strip()
    if env_list:
        models = [m.strip() for m in env_list.split(",") if m.strip()]
        return _unique_keep_order(models)

    single = (os.getenv("GEMINI_MODEL") or "").strip()
    if single:
        return _unique_keep_order([single] + GEMINI_MODELS_DEFAULT)

    return list(GEMINI_MODELS_DEFAULT)


# ‚îÄ‚îÄ –æ–±—â–∞—è –æ–±—ë—Ä—Ç–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def gpt_complete(
    prompt: str,
    system: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 600,
) -> str:
    """–ü—Ä–æ–±—É–µ—Ç –ø–æ –æ—á–µ—Ä–µ–¥–∏: OpenAI ‚Üí Gemini ‚Üí Groq. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç text –∏–ª–∏ ""."""
    if not prompt:
        return ""

    text = ""

    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    # 1) OpenAI (–æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å)
    if "openai" in PROVIDER_ORDER and not text:
        cli = _openai_client()
        if cli:
            try:
                r = cli.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
                text = (r.choices[0].message.content or "").strip()
                if text:
                    log.info("LLM: OpenAI ok (model=%s)", OPENAI_MODEL)
            except Exception as e:
                msg = str(e)
                if _is_quota_or_ratelimit(msg):
                    log.warning("OpenAI error (skip to next): %s", e)
                else:
                    log.warning("OpenAI error: %s", e)
                text = ""

    # 2) Gemini (–ø–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π)
    if "gemini" in PROVIDER_ORDER and not text:
        cli = _gemini_client()
        if cli:
            models_to_try = _gemini_models_to_try()
            avail = _gemini_available_models(cli)
            if avail:
                # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º id, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                filtered = [m for m in models_to_try if m in avail]
                if filtered:
                    models_to_try = filtered
            for mdl in models_to_try:
                try:
                    r = cli.chat.completions.create(
                        model=mdl,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                    text = (r.choices[0].message.content or "").strip()
                    if text:
                        log.info("LLM: Gemini ok (model=%s)", mdl)
                        break
                except Exception as e:
                    msg = str(e)
                    if _is_model_not_found(msg):
                        log.warning("Gemini model %s not found/unsupported, trying next.", mdl)
                        continue
                    if _is_quota_or_ratelimit(msg):
                        log.warning("Gemini quota/rate limit on %s, trying next.", mdl)
                        continue
                    log.warning("Gemini error on %s: %s", mdl, e)
                    continue

    # 3) Groq (–ø–µ—Ä–µ–±–æ—Ä –º–æ–¥–µ–ª–µ–π)
    if "groq" in PROVIDER_ORDER and not text:
        cli = _groq_client()
        if cli:
            for mdl in GROQ_MODELS:
                try:
                    r = cli.chat.completions.create(
                        model=mdl,
                        messages=messages,
                        temperature=temperature,
                        max_tokens=max_tokens,
                    )
                    text = (r.choices[0].message.content or "").strip()
                    if text:
                        log.info("LLM: Groq ok (model=%s)", mdl)
                        break
                except Exception as e:
                    msg = str(e)
                    if _is_model_not_found(msg):
                        log.warning("Groq model %s decommissioned/not found, trying next.", mdl)
                        continue
                    if _is_quota_or_ratelimit(msg):
                        log.warning("Groq rate limit on %s, trying next.", mdl)
                        continue
                    log.warning("Groq error on %s: %s", mdl, e)
                    continue

    return text or ""


# ‚îÄ‚îÄ —Å–ª–æ–≤–∞—Ä–∏ —Ñ–æ–ª–±—ç–∫–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CULPRITS = {
    "—Ç—É–º–∞–Ω": {
        "emoji": "üåÅ",
        "tips": [
            "üî¶ –°–≤–µ—Ç–ª–∞—è –æ–¥–µ–∂–¥–∞ –∏ —Ñ–æ–Ω–∞—Ä—å",
            "üöó –í–æ–¥–∏—Ç–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ",
            "‚è∞ –ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –ø–æ–µ–∑–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ",
            "üï∂Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—á–∫–∏ –ø—Ä–æ—Ç–∏–≤ –±–ª–∏–∫–æ–≤",
        ],
    },
    "–º–∞–≥–Ω–∏—Ç–Ω—ã–µ –±—É—Ä–∏": {
        "emoji": "üß≤",
        "tips": [
            "üßò 5-–º–∏–Ω—É—Ç–Ω–∞—è –¥—ã—Ö–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞",
            "üåø –ó–∞–≤–∞—Ä–∏—Ç–µ —á–∞–π —Å —Ç—Ä–∞–≤–∞–º–∏",
            "üôÖ –ò–∑–±–µ–≥–∞–π—Ç–µ —Å—Ç—Ä–µ—Å—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π",
            "üòå –õ—ë–≥–∫–∞—è —Ä–∞—Å—Ç—è–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–Ω–æ–º",
        ],
    },
    "–Ω–∏–∑–∫–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ": {
        "emoji": "üå°Ô∏è",
        "tips": [
            "üíß –ü–µ–π—Ç–µ –±–æ–ª—å—à–µ –≤–æ–¥—ã",
            "üò¥ 20-–º–∏–Ω—É—Ç–Ω—ã–π –¥–Ω–µ–≤–Ω–æ–π –æ—Ç–¥—ã—Ö",
            "ü§∏ –õ—ë–≥–∫–∞—è –∑–∞—Ä—è–¥–∫–∞ —É—Ç—Ä–æ–º",
            "ü•ó –õ—ë–≥–∫–∏–π —É–∂–∏–Ω –±–µ–∑ —Å–æ–ª–∏",
        ],
    },
    "—à–∞–ª—å–Ω–æ–π –≤–µ—Ç–µ—Ä": {
        "emoji": "üí®",
        "tips": [
            "üß£ –ó–∞—Ö–≤–∞—Ç–∏—Ç–µ —à–∞—Ä—Ñ",
            "üö∂ –ö–æ—Ä–æ—Ç–∫–∞—è –ø—Ä–æ–≥—É–ª–∫–∞",
            "üï∂Ô∏è –ó–∞—â–∏—Ç–∏—Ç–µ –≥–ª–∞–∑–∞ –æ—Ç –ø—ã–ª–∏",
            "üå≥ –ò–∑–±–µ–≥–∞–π—Ç–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤",
        ],
    },
    "–∂–∞—Ä–∞": {
        "emoji": "üî•",
        "tips": [
            "üí¶ –î–µ—Ä–∂–∏—Ç–µ –±—É—Ç—ã–ª–∫—É –≤–æ–¥—ã —Ä—è–¥–æ–º",
            "üß¢ –ù–æ—Å–∏—Ç–µ –≥–æ–ª–æ–≤–Ω–æ–π —É–±–æ—Ä",
            "üå≥ –ò—â–∏—Ç–µ —Ç–µ–Ω—å –≤ –ø–æ–ª–¥–µ–Ω—å",
            "‚ùÑÔ∏è –ü—Ä–æ—Ö–ª–∞–¥–Ω—ã–π –∫–æ–º–ø—Ä–µ—Å—Å –Ω–∞ –ª–æ–±",
        ],
    },
    "—Å—ã—Ä–æ—Å—Ç—å": {
        "emoji": "üíß",
        "tips": [
            "üëü –°–º–µ–Ω–∏—Ç–µ –æ–±—É–≤—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏",
            "üåÇ –î–µ—Ä–∂–∏—Ç–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –∑–æ–Ω—Ç",
            "üå¨Ô∏è –ü—Ä–æ–≤–µ—Ç—Ä–∏–≤–∞–π—Ç–µ –∂–∏–ª–∏—â–µ",
            "üß• –õ—ë–≥–∫–∞—è –Ω–µ–ø—Ä–æ–º–æ–∫–∞–µ–º–∞—è –∫—É—Ä—Ç–∫–∞",
        ],
    },
    "–ø–æ–ª–Ω–∞—è –ª—É–Ω–∞": {
        "emoji": "üåï",
        "tips": [
            "üìù –ó–∞–ø–∏—à–∏—Ç–µ —è—Ä–∫–∏–µ –∏–¥–µ–∏ –ø–µ—Ä–µ–¥ —Å–Ω–æ–º",
            "üßò –ú—è–≥–∫–∞—è –º–µ–¥–∏—Ç–∞—Ü–∏—è –≤–µ—á–µ—Ä–æ–º",
            "üåô –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –ª—É–Ω—É –±–µ–∑ –≥–∞–¥–∂–µ—Ç–æ–≤",
            "üìö –ß—Ç–µ–Ω–∏–µ –Ω–∞ —Å–≤–µ–∂–µ–º –≤–æ–∑–¥—É—Ö–µ",
        ],
    },
    "–º–∏–Ω–∏-–ø–∞—Ä–∞–¥ –ø–ª–∞–Ω–µ—Ç": {
        "emoji": "‚ú®",
        "tips": [
            "üî≠ –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –Ω–µ–±–æ –Ω–∞ —Ä–∞—Å—Å–≤–µ—Ç–µ",
            "üì∏ –°–¥–µ–ª–∞–π—Ç–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –∑–∞–∫–∞—Ç–∞",
            "ü§î –ü–æ–¥—É–º–∞–π—Ç–µ –æ –±–µ—Å–∫—Ä–∞–π–Ω–∏—Ö –ø—Ä–æ—Å—Ç–æ—Ä–∞—Ö",
            "üé∂ –°–ª—É—à–∞–π—Ç–µ —Å–ø–æ–∫–æ–π–Ω—É—é –º—É–∑—ã–∫—É –≤–µ—á–µ—Ä–æ–º",
        ],
    },
}

ASTRO_HEALTH_FALLBACK: List[str] = [
    "üí§ –°–æ–±–ª—é–¥–∞–π—Ç–µ —Ä–µ–∂–∏–º —Å–Ω–∞: –ª–æ–∂–∏—Ç–µ—Å—å –Ω–µ –ø–æ–∑–∂–µ 23:00",
    "ü•¶ –í–∫–ª—é—á–∏—Ç–µ –≤ —Ä–∞—Ü–∏–æ–Ω —Å–≤–µ–∂–∏–µ –æ–≤–æ—â–∏ –∏ –∑–µ–ª–µ–Ω—å",
    "ü•õ –ü–µ–π—Ç–µ —Ç—ë–ø–ª–æ–µ –º–æ–ª–æ–∫–æ —Å –º—ë–¥–æ–º –ø–µ—Ä–µ–¥ —Å–Ω–æ–º",
    "üßò –î–µ–ª–∞–π—Ç–µ –ª—ë–≥–∫—É—é —Ä–∞—Å—Ç—è–∂–∫—É —É—Ç—Ä–æ–º –∏ –≤–µ—á–µ—Ä–æ–º",
    "üö∂ –ü—Ä–æ–≥—É–ª–∏–≤–∞–π—Ç–µ—Å—å 20 –º–∏–Ω—É—Ç –Ω–∞ —Å–≤–µ–∂–µ–º –≤–æ–∑–¥—É—Ö–µ",
]


def gpt_blurb(culprit: str) -> Tuple[str, List[str]]:
    """–ü—É–±–ª–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±–ª–æ–∫–∞ ¬´–í—ã–≤–æ–¥/–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏¬ª."""
    culprit_lower = (culprit or "").lower().strip()

    def _make_prompt(cul: str, astro: bool) -> str:
        if astro:
            return (
                "–î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π health coach —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—ã, "
                "–∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –ø–∏—à–µ—Ç –≥—Ä–∞–º–æ—Ç–Ω–æ. "
                f"–ù–∞–ø–∏—à–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ¬´–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {cul}!¬ª. "
                "–ü–æ—Å–ª–µ —Ç–æ—á–∫–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ–∑–∏—Ç–∏–≤ ‚â§12 —Å–ª–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤. –ù–µ –ø–∏—à–∏ —Å–∞–º–æ —Å–ª–æ–≤–æ ¬´—Å–æ–≤–µ—Ç¬ª. "
                "–ó–∞—Ç–µ–º –¥–∞–π —Ä–æ–≤–Ω–æ 3 —Å–æ–≤–µ—Ç–∞ (—Å–æ–Ω, –ø–∏—Ç–∞–Ω–∏–µ, –¥—ã—Ö–∞–Ω–∏–µ/–ª—ë–≥–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å) "
                "‚â§12 —Å–ª–æ–≤ —Å —ç–º–æ–¥–∑–∏. –û—Ç–≤–µ—Ç ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–∞–º."
            )
        return (
            "–î–µ–π—Å—Ç–≤—É–π –∫–∞–∫ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π health coach —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—ã, "
            "–∫–æ—Ç–æ—Ä—ã–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –∏–∑—É—á–∞–µ—Ç —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –ø–∏—à–µ—Ç –≥—Ä–∞–º–æ—Ç–Ω–æ. "
            f"–ù–∞–ø–∏—à–∏ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ¬´–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {cul}!¬ª. "
            "–ü–æ—Å–ª–µ —Ç–æ—á–∫–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π –ø–æ–∑–∏—Ç–∏–≤ ‚â§12 —Å–ª–æ–≤ –¥–ª—è –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤. "
            "–ó–∞—Ç–µ–º –¥–∞–π —Ä–æ–≤–Ω–æ 3 —Å–æ–≤–µ—Ç–∞ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω–µ "
            "(–ø–∏—Ç–∞–Ω–∏–µ, —Å–æ–Ω, –ª—ë–≥–∫–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å) ‚â§12 —Å–ª–æ–≤ —Å —ç–º–æ–¥–∑–∏. "
            "–ù–µ –ø–∏—à–∏ —Å–∞–º–æ —Å–ª–æ–≤–æ ¬´—Å–æ–≤–µ—Ç¬ª. –û—Ç–≤–µ—Ç ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–∞–º."
        )

    def _from_lines(cul: str, lines: List[str], fallback_pool: List[str]) -> Tuple[str, List[str]]:
        summary = lines[0] if lines else f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {cul}! üòâ"
        tips = [ln for ln in lines[1:] if ln][:3]
        if len(tips) < 2:
            remaining = [t for t in fallback_pool if t not in tips]
            if remaining:
                tips += random.sample(remaining, min(3 - len(tips), len(remaining)))
        return summary, tips[:3]

    if culprit_lower in CULPRITS:
        tips_pool = CULPRITS[culprit_lower]["tips"]
        prompt = _make_prompt(culprit, astro=False)
        text = gpt_complete(prompt=prompt, system=None, temperature=0.7, max_tokens=500)
        if not text:
            summary = f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ"
            return summary, random.sample(tips_pool, min(3, len(tips_pool)))
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, tips_pool)

    astro_keywords = ["–ª—É–Ω–∞", "–Ω–æ–≤–æ–ª—É–Ω–∏–µ", "–ø–æ–ª–Ω–æ–ª—É–Ω–∏–µ", "—á–µ—Ç–≤–µ—Ä—Ç—å"]
    is_astro = any(k in culprit_lower for k in astro_keywords)
    if is_astro:
        prompt = _make_prompt(culprit, astro=True)
        text = gpt_complete(prompt=prompt, system=None, temperature=0.7, max_tokens=500)
        if not text:
            summary = f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ"
            return summary, random.sample(ASTRO_HEALTH_FALLBACK, 3)
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return _from_lines(culprit, lines, ASTRO_HEALTH_FALLBACK)

    prompt = _make_prompt(culprit, astro=True)
    text = gpt_complete(prompt=prompt, system=None, temperature=0.7, max_tokens=500)
    if not text:
        summary = f"–ï—Å–ª–∏ –∑–∞–≤—Ç—Ä–∞ —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫, –≤–∏–Ω–∏—Ç–µ {culprit}! üòâ"
        return summary, random.sample(ASTRO_HEALTH_FALLBACK, 3)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    fallback_pool = ASTRO_HEALTH_FALLBACK + sum((c["tips"] for c in CULPRITS.values()), [])
    return _from_lines(culprit, lines, fallback_pool)
